{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicohim87/DeepLearning/blob/main/Pertemuan5/Session5_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Nicohim87/DeepLearning/refs/heads/main/Pertemuan5/ind2.txt -O ind2.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HQEowSXxnhT",
        "outputId": "d936f30e-fb8c-44df-f2d0-876439059499"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-08 01:38:08--  https://raw.githubusercontent.com/Nicohim87/DeepLearning/refs/heads/main/Pertemuan5/ind2.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815152 (796K) [text/plain]\n",
            "Saving to: ‘ind2.txt’\n",
            "\n",
            "\rind2.txt              0%[                    ]       0  --.-KB/s               \rind2.txt            100%[===================>] 796.05K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2024-10-08 01:38:08 (114 MB/s) - ‘ind2.txt’ saved [815152/815152]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8H_N4rL8u2ZL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI_l0CNhu2ZP",
        "outputId": "98d4f835-8d15-4bd4-e749-5e8ab027941a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "df = tf.data.TextLineDataset('ind2.txt')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "G_Fy3wC7u2ZQ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 22\n",
        "MAX_TOKENS = 20000\n",
        "ENG_SEQ_LEN = 32\n",
        "INA_SEQ_LEN = 32\n",
        "EMBEDDING_DIM = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2cf3yMzsu2ZR"
      },
      "outputs": [],
      "source": [
        "english_vec_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize = \"lower_and_strip_punctuation\",\n",
        "    max_tokens = MAX_TOKENS,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = ENG_SEQ_LEN\n",
        ")\n",
        "\n",
        "indonesian_vec_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize = \"lower_and_strip_punctuation\",\n",
        "    max_tokens = MAX_TOKENS,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = INA_SEQ_LEN\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qfXdAB_Pu2ZR"
      },
      "outputs": [],
      "source": [
        "def split_text(text):\n",
        "    text = tf.strings.split(text, '\\t')\n",
        "    input_1 = text[:1]\n",
        "    input_2 = 'starttoken ' + text[1:2] + ' endtoken'\n",
        "    return input_1, input_2\n",
        "\n",
        "def vectorize(text):\n",
        "    text = tf.strings.split(text, '\\t')\n",
        "    input_1 = text[:1]\n",
        "    input_start = 'starttoken ' + text[1:2]\n",
        "    input_end = text[1:2] + ' endtoken'\n",
        "    return {\n",
        "        'input_1' : english_vec_layer(input_1),\n",
        "        'input_2' : indonesian_vec_layer(input_start)\n",
        "    }, indonesian_vec_layer(input_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oPVF82Gou2ZS"
      },
      "outputs": [],
      "source": [
        "splitted = df.map(split_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "A2XvRwRvu2ZS"
      },
      "outputs": [],
      "source": [
        "eng_data = splitted.map(lambda x, y: x)\n",
        "english_vec_layer.adapt(eng_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QctCsimTu2ZT"
      },
      "outputs": [],
      "source": [
        "ina_data = splitted.map(lambda x, y: y)\n",
        "indonesian_vec_layer.adapt(ina_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MXfPnY56u2ZU"
      },
      "outputs": [],
      "source": [
        "data = df.map(vectorize)\n",
        "data = data.shuffle(200).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "data_len = sum(1 for _ in data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nWJ_mWkDu2ZU"
      },
      "outputs": [],
      "source": [
        "data_len = sum(1 for _ in data)\n",
        "train = df.take(int(data_len * 0.9))\n",
        "validate = df.skip(int(data_len * 0.9))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(units, return_sequences = True)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "    return self.lstm(x)"
      ],
      "metadata": {
        "id": "GixjFUDXu4mg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.w_1 = tf.keras.layers.Dense(units)\n",
        "    self.w_2 = tf.keras.layers.Dense(units)\n",
        "    self.w_output = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, prev_dec_state, enc_state):\n",
        "    scores = self.w_output(tf.nn.tanh(self.w_1(tf.expand_dims(prev_dec_state, -2)) + self.w_2(enc_state)))\n",
        "    attention_weights = tf.nn.softmax(scores, axis=1)\n",
        "    context_vector = attention_weights * enc_state\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "KTFdL5_Kvu_o"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, sequence_length):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.attention = BahdanauAttention(dec_units)\n",
        "    self.gru = tf.keras.layers.GRU(dec_units, return_sequences = True, return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    self.sequence_length = sequence_length\n",
        "\n",
        "  def call(self, x, hidden, shifted_target):\n",
        "    outputs = []\n",
        "    attention_weights = []\n",
        "    shifted_target = self.embedding(shifted_target)\n",
        "\n",
        "    for t in range(0, self.sequence_length):\n",
        "      context_vector, attention_weights = self.attention(hidden, x)\n",
        "      dec_input = context_vector + shifted_target[:, t]\n",
        "      output, hidden = self.gru(tf.expand_dims(dec_input, 1))\n",
        "      outputs.append(output[:, 0])\n",
        "\n",
        "    outputs = tf.convert_to_tensor(outputs)\n",
        "    outputs = tf.transpose(outputs, perm=[1,0,2])\n",
        "    outputs = self.dense(outputs)\n",
        "\n",
        "    return outputs, attention_weights"
      ],
      "metadata": {
        "id": "3xFc6nGLw4Yv"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_UNITS = 256"
      ],
      "metadata": {
        "id": "gAGYHZCCzIqX"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tf.keras.layers.Input(shape=(ENG_SEQ_LEN, ), dtype='int64', name='input_1')\n",
        "encoder = Encoder(english_vec_layer.vocabulary_size(), EMBEDDING_DIM, HIDDEN_UNITS)\n",
        "encoder_output = encoder(input)"
      ],
      "metadata": {
        "id": "KZwFEsab0Dh0"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shifted_target = tf.keras.layers.Input(shape=(INA_SEQ_LEN, ), dtype = 'int64', name='input_2')\n",
        "decoder = Decoder(indonesian_vec_layer.vocabulary_size(), EMBEDDING_DIM, HIDDEN_UNITS, INA_SEQ_LEN)\n",
        "decoder_output, attention_weights = decoder(encoder_output, tf.zeros([1, HIDDEN_UNITS]), shifted_target)"
      ],
      "metadata": {
        "id": "F9W18_ta0ENc",
        "outputId": "0eb6eaec-52ed-42d1-f001-9f8f7eaea48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:1331: UserWarning: Layer 'decoder_3' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Exception encountered when calling GRU.call().\n",
            "\n",
            "\u001b[1mlen is not well defined for a symbolic Tensor (gru_1_1/Squeeze:0). Please call `x.shape` rather than `len(x)` for shape information.\u001b[0m\n",
            "\n",
            "Arguments received by GRU.call():\n",
            "  • sequences=tf.Tensor(shape=(None, 1, 256), dtype=float32)\n",
            "  • initial_state=None\n",
            "  • mask=None\n",
            "  • training=False''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'decoder_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Exception encountered when calling Decoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'decoder_3' (of type Decoder). Either the `Decoder.call()` method is incorrect, or you need to implement the `Decoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling GRU.call().\n\n\u001b[1mlen is not well defined for a symbolic Tensor (gru_1_1/Squeeze:0). Please call `x.shape` rather than `len(x)` for shape information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, 1, 256), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=False\u001b[0m\n\nArguments received by Decoder.call():\n  • args=('<KerasTensor shape=(None, 32, 256), dtype=float32, sparse=False, name=keras_tensor_13>', 'tf.Tensor(shape=(1, 256), dtype=float32)', '<KerasTensor shape=(None, 32), dtype=int64, sparse=None, name=input_2>')\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-f140a378e91f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mshifted_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINA_SEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'int64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindonesian_vec_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_UNITS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINA_SEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_UNITS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshifted_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-22ec1dd36a67>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden, shifted_target)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mdec_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_vector\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshifted_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling Decoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'decoder_3' (of type Decoder). Either the `Decoder.call()` method is incorrect, or you need to implement the `Decoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling GRU.call().\n\n\u001b[1mlen is not well defined for a symbolic Tensor (gru_1_1/Squeeze:0). Please call `x.shape` rather than `len(x)` for shape information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, 1, 256), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=False\u001b[0m\n\nArguments received by Decoder.call():\n  • args=('<KerasTensor shape=(None, 32, 256), dtype=float32, sparse=False, name=keras_tensor_13>', 'tf.Tensor(shape=(1, 256), dtype=float32)', '<KerasTensor shape=(None, 32), dtype=int64, sparse=None, name=input_2>')\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model([input, shifted_target], decoder_output)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yjV2XRzd0Ugl",
        "outputId": "da354202-7cdf-4cab-8df8-bf21ba1efd41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'shifted_target' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-890ba1cedf44>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshifted_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'shifted_target' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparese_categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "history = model.fit(train, validataion_data = validate, epochs=10)"
      ],
      "metadata": {
        "id": "Nnu08zj707Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {(x,y) for x,y in enumerate(indonesian_vec_layer.get_vocabulary())}"
      ],
      "metadata": {
        "id": "K99TXF8D17u4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "  tokenize_input = english_vec_layer([text])\n",
        "\n",
        "  shift_target = ['starttoken ']\n",
        "  results = ''\n",
        "  for i in range(INA_SEQ_LEN):\n",
        "    tokenize_shifted = indonesian_vec_layer([' '.join(shift_target)])\n",
        "    output = model.predict([tokenize_input], tokenize_shifted)\n",
        "    word_idx = tf.argmax(output, axis=-1)[0][i].numpy()\n",
        "    current_word = index_to_word[word_idx]\n",
        "    if current_word == 'stoptoken':\n",
        "      break\n",
        "    shift_target.append(current_word)\n",
        "    results += ' ' + current_word if results else current_word\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "4xDpySVJ1NyV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}